import { Container, Header, SpaceBetween } from '@cloudscape-design/components';
import React, { useEffect } from 'react';
import { useParams } from 'react-router-dom';

import { ExternalLink } from '/addons/details/external-link';
import { selectArticle } from '/data/articles';
import { useAppSelector } from '/data/data-store';

/* eslint-disable max-lines-per-function */
export const SyntheticData = () => {
  const params = useParams();
  const article = useAppSelector(selectArticle(params.slug!));

  useEffect(() => {
    setTimeout(() => {
      if (window.Prism) window.Prism.highlightAll();
    }, 0);
  }, []);

  return <SpaceBetween size="l">
    <Container
      header={<Header variant="h2">Synthetic Data: Unlocking the Potential of Artificial Intelligence</Header>}
      media={{
        content: <img src={article?.image} alt="" />
      }}>
      <SpaceBetween size="m">
        <p>
        In our ever-evolving world of technology, synthetic data is emerging as a game-changer for AI development. As we navigate the complexities of AI model training and testing, synthetic data offers an exciting solution to overcome data scarcity, bias, and costs associated with collecting high-quality datasets.
        </p>
        <p>
        But what exactly is synthetic data? Simply put, it's artificial data generated by machine learning algorithms that mimics real-world patterns. By harnessing the power of Large Language Models (LLMs), researchers can produce massive amounts of training data without the need for real-world examples. This allows developers to accelerate AI model development and testing in a more controlled environment.
        </p>
        <h3>Challenges of Synthetic Data</h3>
        <p>
        Google DeepMind
          <br />Stanford University
          <br />Georgia Institute of Technology
        </p>
        <h3>Why</h3>
        <p>
        While synthetic data is a promising innovation, its limitations should not be overlooked. One primary concern lies with "hallucinations," where models replicate errors present in their training datasets rather than producing original content. These hallmarks are embedded into the AI itself and may lead to unreliable outputs in production.
        </p>
        <p>
        To combat these issues, it's essential for developers to carefully select algorithms and evaluate model behavior across diverse settings to minimize adverse consequences.
        </p>
        <p>
        Another pitfall is bias; LLMs inherit their flaws if data comes from preconceptions rather than raw facts. Even more serious concerns arise with "impersonation," as we have already discussed in detail how it affects people on social networks when bots mimic the content they're spreading across different social networks. Synthetic datasets generated to promote this fake activity is hazardous as they produce even larger quantities of biased and distorted data for all involved AI model types. When synthetic datasets aren't used properly, potential results will show in every future algorithm output. If LLMs behave unnaturally after such training methods, we see not a useful product but another bad application that misalign with values.
        </p>
      </SpaceBetween>
    </Container>
    <Container header={<Header variant="h2">Benefits and Best Practices</Header>}>
      <SpaceBetween size="m">
        <p>
        Despite these risks, employing synthetic data has considerable advantages in areas where:
        </p>
        <ul>
          <li>
          Sensitivity and safety come first, (financial or medical).
          </li>
          <li>
          Testing unusual, but necessary cases. Think of critical, high impact scenarios which aren't rare for us anymore due AI capabilities (like driving at night).
          </li>
        </ul>
        <p>
        This will significantly shorten testing timelines. Even better real-world costs drop faster.
        </p>
        <p>
        Stick to actual information sources like online users or websites for sensitive, social decision products when using human values.
        </p>
      </SpaceBetween>
    </Container>
    <Container header={<Header variant="h2">Benefits for Developers and Organizations</Header>}>
      <SpaceBetween size="m">
        <p>
        As mentioned in the book "Building AI-Powered Products", we've established several critical scenarios that might demand the creation of artificial information. One more major gain comes with accelerated prototyping for startups or established enterprises aiming rapid releases â€“ early access means a better ROI than collecting vast resources for lengthy process completion.
        </p>
        <p>
        To summarize - synthetic datasets give control to testing without costs in order to make development less expensive.
        </p>
        <p>
        Let's walk you through setting up one easy synthetic example with python.
        </p>
        <p>
        Below I provide a basic usage of scikit-learn with Python. The data it generates are just blobs on 2D graph.
        </p>
      </SpaceBetween>
    </Container>
    <Container header={<Header variant="h2">Python Example: Using Scikit-learn</Header>}>
      <SpaceBetween size="m">
        <p>
        Before you begin, you will need to install the sklearn package. It's recommended to also use a virtual environment (venv).
        </p>
        <pre><code className="language-bash">{`
$ python -m venv sklearn-env
$ source sklearn-env/bin/activate  # activate
$ pip install -U scikit-learn matplotlib
      `}</code></pre>
        <p>
          <code>sci-learn</code> has many different methods for creating synthetic data. <code>make_regression</code> and <code>make_classification</code> are two of the most valuable, but this example focuses on <code>make_blobs</code>. With <code>make_blobs</code> the <code>centers</code> parameter controls the number of clustered data points to be generated.
        </p>
        <pre><code className="language-python">{`
from sklearn.datasets import make_blobs

features, target = make_blobs(
  n_samples = 100,
  n_features = 2,
  centers = 4,
  cluster_std = 0.3,
  shuffle = True,
  random_state = 1
)

print('features ', features[:3])
print('target ', target[:3])
      `}</code></pre>
        <p>
          We can then see the distibution of clusters by using the <code>matplotlib</code> library.
        </p>
        <pre><code className="language-python">{`
import matplotlib.pyplot as plot

plot.scatter(features[:,0], features[:,1], c=target)
plot.show()
      `}</code></pre>
        <img style={{ maxWidth: 500 }} src="https://bishopz.s3.amazonaws.com/2024/scilearn-distribution.webp" alt="graph showing dot plot distribution in four clusters" />
      </SpaceBetween>
    </Container>
  </SpaceBetween>;
};
