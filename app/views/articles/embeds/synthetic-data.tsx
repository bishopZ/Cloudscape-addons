import { Container, Header, SpaceBetween } from '@cloudscape-design/components';
import React, { useEffect } from 'react';
import { useParams } from 'react-router-dom';

import { selectArticle } from '/data/articles';
import { useAppSelector } from '/data/data-store';

/* eslint-disable max-lines-per-function */
export const SyntheticData = () => {
  const params = useParams();
  const article = useAppSelector(selectArticle(params.slug!));

  useEffect(() => {
    setTimeout(() => {
      if (window.Prism) window.Prism.highlightAll();
    }, 0);
  }, []);

  return <SpaceBetween size="l">
    <Container
      header={<Header variant="h2">Challenges of Synthetic Data</Header>}
      media={{
        content: <img src={article?.image} alt="" />
      }}>
      <SpaceBetween size="m">
        <p>
        In our ever-evolving world of technology, synthetic data is emerging as a game-changer for AI development. As we navigate the complexities of AI model training and testing, synthetic data offers an exciting solution to overcome data scarcity, bias, and costs associated with collecting high-quality datasets.
        </p>
        <p>
        But what exactly is synthetic data? Simply put, it's artificial data generated by computer programs that mimics real-world patterns. By harnessing the power of Large Language Models (LLMs), researchers can produce massive amounts of training data without the need for real-world examples. This allows developers to accelerate AI model development and testing in a more controlled environment.
        </p>
        <p>
        While synthetic data is a promising innovation, its limitations should not be overlooked. One primary concern lies with "hallucinations," where models replicate errors present in their training datasets rather than producing original content. These hallmarks are embedded into the AI itself and may lead to unreliable outputs in production.
        </p>
        <p>
        To combat these issues, developers should carefully select algorithms and evaluate model behavior across diverse settings to minimize adverse consequences.
        </p>
        <p>
        Another pitfall is bias; LLMs inherit their flaws if data comes from preconceptions rather than raw facts. Even more serious concerns arise with "impersonation," as we have already discussed in detail how it affects people on social networks when bots mimic the content they're spreading across different social networks. Synthetic datasets generated to promote this fake activity is hazardous as they produce even larger quantities of biased and distorted data for all involved AI model types. When synthetic datasets aren't used properly, potential results will show in every future algorithm output. If LLMs behave unnaturally after such training methods, we see not a useful product, but another bad application that misaligns with our values.
        </p>
      </SpaceBetween>
    </Container>
    <Container header={<Header variant="h2">Benefits</Header>}>
      <SpaceBetween size="m">
        <p>
        In the book, Building AI-Powered Products, Dr. Marily Nika provides an excellent summary of when to use synthetic data.
        </p>
        <p>
        "Use synthetic data when...
        </p>
        <ul>
          <li>
            You're dealing with sensitive information (as in healthcare or financial services).
          </li>
          <li>
            You need to test rare but critical scenarios (such as accidents for self-driving cars).
          </li>
          <li>
            You're in early development and need to move fast.
          </li>
          <li>
            Real data collection could be prohibitively expensive."
          </li>
        </ul>
        <p>
        "Stick to real data when...
        </p>
        <ul>
          <li>
            User behavior and preferences are central to your product.
          </li>
          <li>
            You need to capture cultural or contextual nuances.
          </li>
          <li>You're making high-stakes decisions that directly affect users."</li>
        </ul>
      </SpaceBetween>
    </Container>
    <Container header={<Header variant="h2">Python Example: Using Scikit-learn</Header>}>
      <SpaceBetween size="m">
        <p>
        Let's walk you through setting up one easy synthetic example with python. Below I provide a basic usage of scikit-learn with Python. The data it generates are just blobs on a 2D graph.
        </p>
        <p>
        Before you begin, you'll need to install the sklearn package. It's recommended to also use a virtual environment (venv).
        </p>
        <pre><code className="language-bash">{`
$ python -m venv sklearn-env
$ source sklearn-env/bin/activate  # activate
$ pip install -U scikit-learn matplotlib
      `}</code></pre>
        <p>
          <code>sci-learn</code> has many different methods for creating synthetic data. <code>make_regression</code> and <code>make_classification</code> are two of the most valuable, but this example focuses on <code>make_blobs</code>. With <code>make_blobs</code> the <code>centers</code> parameter controls the number of clustered data points to be generated.
        </p>
        <pre><code className="language-python">{`
from sklearn.datasets import make_blobs

features, target = make_blobs(
  n_samples = 100,
  n_features = 2,
  centers = 4,
  cluster_std = 0.3,
  shuffle = True,
  random_state = 1
)

print('features ', features[:3])
print('target ', target[:3])
      `}</code></pre>
        <p>
          We can then see the distibution of clusters by using the <code>matplotlib</code> library.
        </p>
        <pre><code className="language-python">{`
import matplotlib.pyplot as plot

plot.scatter(features[:,0], features[:,1], c=target)
plot.show()
      `}</code></pre>
        <img style={{ maxWidth: 500 }} src="/images/scilearn-distribution.webp" alt="graph showing dot plot distribution in four clusters" />
      </SpaceBetween>
    </Container>
  </SpaceBetween>;
};
